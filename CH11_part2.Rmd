---
title: "CH 11: Assumptions - Part II"
output: pdf_document
---

\renewcommand{\vec}[1]{\mathbf{#1}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height = 4, fig.width = 6, fig.align = 'center')
library(tidyverse) 
library(rstanarm)
library(arm)
library(gridExtra)
set.seed(10312020)
```

### Posterior Predictive Checks

A another way of understanding the model fit is to use posterior predictive checks. 

\vfill

Consider the candy dataset.
```{r, echo = T, message = F}
candy <- read_csv("https://math.montana.edu/ahoegh/teaching/stat446/candy-data.csv") %>% 
  mutate(pricepercent = pricepercent - mean(pricepercent),
         sugarpercent = sugarpercent - mean(sugarpercent))

candy_model <- stan_glm(winpercent ~ chocolate * caramel +
                        peanutyalmondy+ sugarpercent, data = candy, refresh = 0)

prediction_wins <- posterior_predict(candy_model, data = candy)
```


We can visually compare the simulated datasets with the true dataset.

```{r}
tibble(win = c(candy$winpercent,prediction_wins[1,]), type = rep(c('data','sim1'), each = nrow(candy))) %>% 
  ggplot(aes(x =win, fill = type)) + geom_histogram(bins=20) + 
  facet_wrap(.~type)  + theme_bw() + 
  theme(legend.position = 'none')
  
```

\newpage

```{r, echo = F}
tibble(win = c(candy$winpercent,as.numeric(t(prediction_wins[1:8,]))
), type = rep(c('data',paste('sim', 1:8)), each = nrow(candy))) %>% 
  ggplot(aes(x =win, fill = type)) + geom_histogram(bins=20) + 
  facet_wrap(.~type) + theme_bw() + 
  theme(legend.position = 'none')
```

In addition to the visual inspection of the distributions of the data, we can also look at summary statistics from the simulations vs the observed data.

From the simulations, the minimum value of the simulation is less than the observed minimum value 

Similarly, the maximum value of the simulation is greater than the observed maximum value 


\newpage

```{r}
tibble(x = apply(prediction_wins,1, min)) %>%
  ggplot(aes(x = x)) + geom_histogram(bins = 50) + 
  theme_bw() + ggtitle('Minimum winning percentage from posterior predictive distributions') +
  geom_vline(xintercept = min(candy$winpercent), col = 'red') + 
  annotate('text', x = 0, y = 280, label ='Minimum Value from Dataset', color = 'red') +
  annotate('segment', x = 0, xend = 22, y =270, yend = 250, arrow = arrow(), color = 'red')
```

### Residual Standard Deviation and explained variance ($R^2$)



\vfill

The coefficient of determination, 
$$R^2 = 1 - \frac{\hat{\sigma}^2}{s^2_y}$$

\vfill
At the extreme values, 
\vfill
At the other extreme, 
 
 \vfill
Note that the $R^2$ value does not account for the number of predictors in the model

\newpage

#### Bayesian $R^2$

Conceptually, $R^2$ can be constructed as $\left(\frac{\text{Explained Variance}}{\text{Explained Variance + Residual Variance}}\right)$. 

\vfill

Using this framework, a Bayesian analog can be defined as 
$$\text{Bayesian }R^2_s= \frac{V (\hat{y}_i^s)}{V (\hat{y}_i^s) + \sigma^2_s},$$
where $V (\hat{y}_i^s)$ is the variance of the predicted values for simulation $s$.

\vfill

```{r}
bayes_R2(candy_model) %>% head()
bayes_R2(candy_model) %>% mean() %>% round(3)
```


#### Cross-Validation

Another way to compare models is based on the predictive ability of the model. 

\vfill

One way to do this uses cross-validation, where a chunk of the data is removed from the data for prediction.

\vfill


\vfill

From a classical perspective, or in many machine learning scenarios, it may make sense to compare point predictions with something like 

\newpage

AIC (Akaike information criteria) is a common method to compare models. This uses the likelihood function of the model fit but includes a penalty for additional parameters in the model.

\vfill

Using a Bayesian framework, it can be useful to incorporate the uncertainty in the predictions. Formally this is done using the predicted distribution 
$p(y_i|\beta, \sigma) = \frac{1}{\sqrt{2 \pi}\sigma}\exp \left(- \frac{1}{2 \sigma^2}\left( y_i - X_i \beta \right)^2 \right).$ 
\vfill

```{r}
sugar <- stan_glm(winpercent ~ sugarpercent, data = candy, refresh = 0)
loo1 <- loo(sugar)

chocolate <- stan_glm(winpercent ~ chocolate, data = candy, refresh = 0)
loo2 <- loo(chocolate)
print(loo_compare(loo2, loo1))
```

\newpage

```{r}
k1 <- kfold(sugar, K = 5)
k2 <- kfold(chocolate, K = 5)
loo_compare(k1, k2)
```

